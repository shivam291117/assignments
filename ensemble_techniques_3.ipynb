{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Random Forest Regressor?\n",
        "\n",
        "A Random Forest Regressor is an ensemble learning method that combines multiple decision trees to improve predictive performance and control overfitting. It constructs multiple decision trees during training and outputs the mean prediction of the individual trees for regression tasks.\n",
        "\n",
        "### Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
        "\n",
        "Random Forest Regressor reduces the risk of overfitting by:\n",
        "- **Bootstrapping (Bagging):** Training each tree on a different random subset of the data with replacement, which introduces variability.\n",
        "- **Feature Randomness:** At each split in the decision trees, only a random subset of features is considered for splitting, which decorrelates the trees.\n",
        "- **Averaging Predictions:** The final prediction is an average of the predictions from all the trees, which smooths out the noise and reduces variance.\n",
        "\n",
        "### Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
        "\n",
        "The Random Forest Regressor aggregates predictions by:\n",
        "- Training multiple decision trees on different bootstrap samples of the dataset.\n",
        "- Each tree independently makes a prediction.\n",
        "- The final prediction is the average (mean) of all the individual tree predictions.\n",
        "\n",
        "### Q4. What are the hyperparameters of Random Forest Regressor?\n",
        "\n",
        "Key hyperparameters of Random Forest Regressor include:\n",
        "- **n_estimators:** Number of trees in the forest.\n",
        "- **max_depth:** Maximum depth of each tree.\n",
        "- **min_samples_split:** Minimum number of samples required to split an internal node.\n",
        "- **min_samples_leaf:** Minimum number of samples required to be at a leaf node.\n",
        "- **max_features:** Number of features to consider when looking for the best split.\n",
        "- **bootstrap:** Whether bootstrap samples are used when building trees.\n",
        "- **random_state:** Seed used by the random number generator.\n",
        "- **max_samples:** Number of samples to draw from the total dataset to train each base estimator.\n",
        "\n",
        "### Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
        "\n",
        "**Random Forest Regressor:**\n",
        "- Combines predictions from multiple decision trees to improve performance and reduce overfitting.\n",
        "- Uses bootstrapping and feature randomness to create a diverse set of trees.\n",
        "- Produces a more robust and stable prediction by averaging the outputs of all trees.\n",
        "\n",
        "**Decision Tree Regressor:**\n",
        "- A single decision tree model that splits the data based on feature values to make predictions.\n",
        "- Prone to overfitting, especially with deep trees or noisy data.\n",
        "- Provides a clear, interpretable model but may lack accuracy and generalization compared to ensemble methods like Random Forest.\n",
        "\n",
        "### Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
        "\n",
        "**Advantages:**\n",
        "- **Reduced Overfitting:** By averaging multiple trees, the model is less likely to overfit.\n",
        "- **Robustness:** Handles missing values and maintains performance even with noisy data.\n",
        "- **Feature Importance:** Provides insights into feature importance.\n",
        "- **Scalability:** Efficient with large datasets and can handle many input variables.\n",
        "\n",
        "**Disadvantages:**\n",
        "- **Complexity:** Less interpretable than a single decision tree due to the ensemble nature.\n",
        "- **Computationally Intensive:** Requires more computational resources and memory.\n",
        "- **Parameter Tuning:** Needs careful tuning of hyperparameters for optimal performance.\n",
        "\n",
        "### Q7. What is the output of Random Forest Regressor?\n",
        "\n",
        "The output of a Random Forest Regressor is a continuous value, which is the average of the predictions made by all the decision trees in the ensemble for regression tasks.\n",
        "\n",
        "### Q8. Can Random Forest Regressor be used for classification tasks?\n",
        "\n",
        "Yes, Random Forest can be used for classification tasks. In that case, it is referred to as Random Forest Classifier. The main difference is that instead of averaging the predictions for regression, it takes the majority vote of the predictions from all trees to determine the final class label."
      ],
      "metadata": {
        "id": "Xbq3tkOtlpvG"
      }
    }
  ]
}