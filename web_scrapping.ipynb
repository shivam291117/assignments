{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Web Scraping?\n",
        "\n",
        "Why is it Used?\n",
        "\n",
        "Give three areas where Web Scraping is used to get data."
      ],
      "metadata": {
        "id": "BpuSuUchQA2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web scraping is the process of extracting data from websites. This data extraction is usually performed by a software tool or a script that navigates web pages, retrieves relevant information, and then transforms this data into a structured format such as a spreadsheet or a database.\n",
        "\n",
        "\n",
        "**Why is Web Scraping Used?**\n",
        "\n",
        "\n",
        "Data Collection: Web scraping allows for automated and efficient collection of large amounts of data from the web, which can then be analyzed and used for various purposes.\n",
        "\n",
        "\n",
        "Automation: It automates the tedious task of manually gathering data from multiple sources, saving time and effort.\n",
        "\n",
        "\n",
        "Competitive Analysis: Companies use web scraping to monitor competitors, track prices, and stay updated with market trends.\n",
        "\n",
        "\n",
        "Research: Researchers and analysts use web scraping to gather data for academic and business research.\n",
        "\n",
        "\n",
        "\n",
        "**Three Areas Where Web Scraping is Used**\n",
        "\n",
        "\n",
        "\n",
        "Price Monitoring:\n",
        "\n",
        "Companies scrape competitor websites to monitor pricing and adjust their own prices accordingly.\n",
        "Product Information: Aggregators gather product details, reviews, and ratings from various e-commerce platforms.\n",
        "Real Estate:\n",
        "\n",
        "\n",
        "\n",
        "Property Listings:\n",
        "\n",
        " Websites like Zillow and Realtor.com can be scraped to collect data on property listings, prices, and trends.\n",
        "Market Analysis: Real estate analysts use scraped data to analyze market conditions, rental prices, and housing availability.\n",
        "Social Media:\n",
        "\n",
        "\n",
        "\n",
        "Sentiment Analysis:\n",
        "\n",
        " Brands scrape social media platforms to gather data on user opinions and sentiments about their products and services.\n",
        "Trend Tracking: Marketers use scraping to identify trending topics and hashtags, helping them to create relevant and timely content."
      ],
      "metadata": {
        "id": "AWGu7l9wQMgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the different methods used for Web Scraping?\n",
        "\n",
        "Ans.\n",
        "\n",
        "Web Scrapers can be divided on the basis of many different criteria, including Self-built or Pre-built Web Scrapers, Browser extension or Software Web Scrapers, and Cloud or Local Web Scrapers. You can have Self-built Web Scrapers but that requires advanced knowledge of programming"
      ],
      "metadata": {
        "id": "ovUx2zjHQjTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is Beautiful Soup? Why is it used?\n",
        "\n",
        "Ans.\n",
        "\n",
        "Beautiful Soup is a Python package for parsing HTML and XML documents, including those with malformed markup. It creates a parse tree for documents that can be used to extract data from HTML, which is useful for web scraping."
      ],
      "metadata": {
        "id": "QbZY2ckDQqmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Why is flask used in this Web Scraping project?\n",
        "\n",
        "Ans.\n",
        "\n",
        "Flask provides a simple and consistent interface to the incoming HTTP request data. From accessing form data, file uploads, cookies, and headers to handling JSON data, Flask's request handling capabilities make it easy to build robust and secure web applications."
      ],
      "metadata": {
        "id": "USkO4wJUQxAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
        "\n",
        "Ans.\n",
        "\n",
        "Amazon EC2 (Elastic Compute Cloud):\n",
        "\n",
        "Provides scalable virtual servers for running your image scraper software.\n",
        "Amazon S3 (Simple Storage Service):\n",
        "\n",
        "Ideal for storing scraped images due to its scalability and durability.\n",
        "Useful for storing other related data, such as logs or metadata.\n",
        "Amazon RDS (Relational Database Service):\n",
        "\n",
        "Used for storing structured data, such as image URLs and metadata, in a relational database like MySQL, PostgreSQL, or Aurora.\n",
        "Amazon DynamoDB:\n",
        "\n",
        "A NoSQL database service that can be used to store metadata and other semi-structured data.\n",
        "AWS Lambda:\n",
        "\n",
        "Enables running code without provisioning or managing servers.\n",
        "Can be used for processing images, such as resizing or tagging images, as they are scraped.\n",
        "Amazon Rekognition:\n",
        "\n",
        "Provides image and video analysis services, such as object detection, facial recognition, and labeling.\n",
        "Can be used to analyze the content of the images you scrape."
      ],
      "metadata": {
        "id": "thvHySxSQ5Fc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6OZUkkEP9yN"
      },
      "outputs": [],
      "source": []
    }
  ]
}