{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
        "\n",
        "We need to find \\( P(\\text{Smoker} | \\text{Health Insurance}) \\).\n",
        "\n",
        "Given:\n",
        "- \\( P(\\text{Health Insurance}) = 0.70 \\)\n",
        "- \\( P(\\text{Smoker} | \\text{Health Insurance}) = 0.40 \\)\n",
        "\n",
        "By definition, \\( P(\\text{Smoker} | \\text{Health Insurance}) \\) is already given as 0.40. Therefore:\n",
        "\n",
        "\\[ P(\\text{Smoker} | \\text{Health Insurance}) = 0.40 \\]\n",
        "\n",
        "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "\n",
        "**Bernoulli Naive Bayes:**\n",
        "- Suitable for binary/Boolean features (0 or 1, indicating absence or presence of a feature).\n",
        "- Considers binary occurrence of features in the document.\n",
        "- Often used for text classification tasks where the presence or absence of a word is considered.\n",
        "\n",
        "**Multinomial Naive Bayes:**\n",
        "- Suitable for discrete features (e.g., word counts in a document).\n",
        "- Considers the frequency of features in the document.\n",
        "- Often used for text classification tasks where the frequency of words is important.\n",
        "\n",
        "### Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "Bernoulli Naive Bayes does not inherently handle missing values. If a feature is missing, it is typically treated as a zero (indicating absence of the feature). However, this might not always be appropriate, so it is common to impute missing values before applying the Bernoulli Naive Bayes classifier.\n",
        "\n",
        "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "\n",
        "Yes, Gaussian Naive Bayes can be used for multi-class classification. It calculates the probability of each class given the feature values and selects the class with the highest probability.\n",
        "\n",
        "### Q5. Assignment\n",
        "\n",
        "#### Data preparation:\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository.\n",
        "\n",
        "#### Implementation:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"spambase.data\", header=None)\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling the features for GaussianNB\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the classifiers\n",
        "classifiers = {\n",
        "    'BernoulliNB': BernoulliNB(),\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'GaussianNB': GaussianNB()\n",
        "}\n",
        "\n",
        "# Train and evaluate the classifiers\n",
        "results = {}\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    if name == 'GaussianNB':\n",
        "        clf.fit(X_train_scaled, y_train)\n",
        "        y_pred = clf.predict(X_test_scaled)\n",
        "    else:\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "# Output the results\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n",
        "```\n",
        "\n",
        "#### Results:\n",
        "- Reported performance metrics: Accuracy, Precision, Recall, F1 score for each classifier.\n",
        "\n",
        "#### Discussion:\n",
        "- Discuss which variant of Naive Bayes performed the best.\n",
        "- Bernoulli Naive Bayes may perform better for binary/Boolean features, Multinomial Naive Bayes for word counts, and Gaussian Naive Bayes for continuous features.\n",
        "- Limitations of Naive Bayes include the assumption of feature independence and sensitivity to zero probabilities.\n",
        "\n",
        "#### Conclusion:\n",
        "- suggestions for future work, such as using feature selection or engineering techniques, or trying other classification algorithms like logistic regression or support vector machines.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xbq3tkOtlpvG"
      }
    }
  ]
}