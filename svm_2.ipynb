{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Let's go through each question in detail.\n",
        "\n",
        "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
        "\n",
        "In machine learning, especially in Support Vector Machines (SVMs), kernel functions are used to enable the algorithm to learn non-linear decision boundaries by implicitly mapping input features into higher-dimensional spaces.\n",
        "\n",
        "**Polynomial functions** are a specific type of kernel function. The polynomial kernel is used to map the data into a higher-dimensional space where a linear decision boundary can be used to separate classes that are not linearly separable in the original feature space.\n",
        "\n",
        "The polynomial kernel function is given by:\n",
        "\n",
        "\\[ K(x, x') = (x \\cdot x' + c)^d \\]\n",
        "\n",
        "where:\n",
        "- \\( x \\) and \\( x' \\) are input vectors.\n",
        "- \\( c \\) is a constant term.\n",
        "- \\( d \\) is the degree of the polynomial.\n",
        "\n",
        "This kernel allows SVMs to learn decision boundaries that are polynomial curves, hence the name \"polynomial kernel\".\n",
        "\n",
        "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
        "\n",
        "To implement an SVM with a polynomial kernel in Scikit-learn, you can use the `SVC` class with the `kernel='poly'` parameter. Here’s how you can do it:\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_clusters_per_class=1, n_redundant=0, random_state=42)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM with a polynomial kernel\n",
        "svc_poly = SVC(kernel='poly', degree=3, C=1)\n",
        "svc_poly.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svc_poly.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Plot decision boundary\n",
        "def plot_decision_boundary(clf, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                         np.arange(y_min, y_max, 0.01))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
        "    plt.title('Polynomial Kernel SVM')\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(svc_poly, X_test, y_test)\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- We use a polynomial kernel with a degree of 3.\n",
        "- We plot the decision boundary to visualize how the polynomial kernel separates the classes.\n",
        "\n",
        "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
        "\n",
        "In Support Vector Regression (SVR), the parameter \\( \\epsilon \\) defines a margin of tolerance where no penalty is given for errors within this margin. Increasing the value of \\( \\epsilon \\) means that the model will be more tolerant to deviations from the true values within this margin, resulting in:\n",
        "\n",
        "- **Fewer Support Vectors**: With a larger \\( \\epsilon \\), more data points will fall within the tolerance margin, reducing the number of support vectors.\n",
        "- **Simpler Model**: The model may become simpler as it fits within a larger margin, potentially reducing the risk of overfitting.\n",
        "\n",
        "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
        "\n",
        "**1. Kernel Function:**\n",
        "   - **Linear Kernel**: Suitable for linearly separable problems or when you believe the relationship is linear.\n",
        "   - **Polynomial Kernel**: Useful for problems where the relationship is polynomial. Increasing the degree can capture more complex relationships but may lead to overfitting.\n",
        "   - **RBF Kernel**: Effective for capturing complex relationships by mapping data to a higher-dimensional space. Suitable for most problems but requires careful tuning of the gamma parameter.\n",
        "\n",
        "**2. C Parameter:**\n",
        "   - **Description**: The regularization parameter \\( C \\) controls the trade-off between achieving a low error on the training data and minimizing the model complexity.\n",
        "   - **High \\( C \\)**: Focuses on minimizing training error, which may lead to overfitting.\n",
        "   - **Low \\( C \\)**: Allows for a larger margin, which may lead to underfitting but can be beneficial for generalization.\n",
        "\n",
        "   **Example**: If your model is overfitting, you might decrease \\( C \\) to reduce the model’s sensitivity to the training data.\n",
        "\n",
        "**3. Epsilon Parameter:**\n",
        "   - **Description**: Defines the margin of tolerance where no penalty is given for errors. It essentially controls the width of the epsilon-insensitive zone.\n",
        "   - **High \\( \\epsilon \\)**: More tolerance for errors within the margin, leading to fewer support vectors and a potentially simpler model.\n",
        "   - **Low \\( \\epsilon \\)**: Less tolerance for errors, potentially increasing the number of support vectors and making the model more complex.\n",
        "\n",
        "   **Example**: If your model is too complex or overfitting, you might increase \\( \\epsilon \\) to simplify the model.\n",
        "\n",
        "**4. Gamma Parameter (for RBF Kernel):**\n",
        "   - **Description**: Controls the influence of a single training example. The higher the gamma, the closer other examples must be to be affected.\n",
        "   - **High Gamma**: Leads to a more flexible model that captures more complexity but may overfit.\n",
        "   - **Low Gamma**: Results in a smoother decision boundary but might underfit.\n",
        "\n",
        "   **Example**: If the decision boundary is too wiggly or overfitting, you might decrease gamma.\n",
        "\n",
        "### Summary\n",
        "- **Kernel Function**: Choose based on the nature of the data (linear, polynomial, or RBF).\n",
        "- **C Parameter**: Controls trade-off between training error and model complexity.\n",
        "- **Epsilon Parameter**: Defines tolerance for errors and affects the number of support vectors.\n",
        "- **Gamma Parameter**: Influences the flexibility of the decision boundary in RBF kernels."
      ],
      "metadata": {
        "id": "mBzMUmWqxjNQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCe5LfK2xc0-"
      },
      "outputs": [],
      "source": [
        "Q5. Assignment:\n",
        "L Import the necessary libraries and load the dataseg\n",
        "L Split the dataset into training and testing setZ\n",
        "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
        "L Create an instance of the SVC classifier and train it on the training datW\n",
        "L hse the trained classifier to predict the labels of the testing datW\n",
        "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
        "precision, recall, F1-scoreK\n",
        "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
        "improve its performanc_\n",
        "L Train the tuned classifier on the entire dataseg\n",
        "L Save the trained classifier to a file for future use.\n",
        "\n",
        "You can use any dataset of your choice for this assignment, but make sure it is suitable for\n",
        "classification and has a sufficient number of features and samples."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import joblib\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data, and transform the testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the SVC classifier\n",
        "svc = SVC()\n",
        "\n",
        "# Train the classifier\n",
        "svc.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict the labels of the testing set\n",
        "y_pred = svc.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'poly', 'rbf'],\n",
        "    'degree': [3, 4]  # Only for polynomial kernel\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Score: {grid_search.best_score_}')\n",
        "\n",
        "# Get the best classifier from grid search\n",
        "best_svc = grid_search.best_estimator_\n",
        "\n",
        "# Train the classifier on the entire dataset\n",
        "best_svc.fit(X, y)\n",
        "\n",
        "# Save the trained classifier to a file\n",
        "joblib.dump(best_svc, 'trained_svc_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD_NDbp4yh1K",
        "outputId": "5e5e7756-8318-4b55-9725-0b63cf21ca5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "Best Parameters: {'C': 10, 'degree': 3, 'kernel': 'linear'}\n",
            "Best Score: 0.9523809523809523\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['trained_svc_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1xuXOA81y2ZC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}