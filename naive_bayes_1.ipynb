{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Bayes' theorem?\n",
        "\n",
        "Bayes' theorem is a fundamental theorem in probability theory that describes the probability of an event based on prior knowledge of conditions that might be related to the event. It provides a way to update the probability estimate for an event as new evidence or information is obtained.\n",
        "\n",
        "### Q2. What is the formula for Bayes' theorem?\n",
        "\n",
        "The formula for Bayes' theorem is:\n",
        "\n",
        "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
        "\n",
        "where:\n",
        "- \\( P(A|B) \\) is the posterior probability of event \\( A \\) given event \\( B \\).\n",
        "- \\( P(B|A) \\) is the likelihood of event \\( B \\) given event \\( A \\).\n",
        "- \\( P(A) \\) is the prior probability of event \\( A \\).\n",
        "- \\( P(B) \\) is the marginal probability of event \\( B \\).\n",
        "\n",
        "### Q3. How is Bayes' theorem used in practice?\n",
        "\n",
        "Bayes' theorem is used in various fields and applications, including:\n",
        "\n",
        "1. **Medical Diagnosis:** Updating the probability of a disease given the presence of certain symptoms.\n",
        "2. **Spam Filtering:** Determining the probability that an email is spam based on the presence of certain words.\n",
        "3. **Machine Learning:** Used in classification algorithms such as Naive Bayes classifiers.\n",
        "4. **Decision Making:** Updating beliefs in the light of new evidence in fields like finance, forensics, and risk management.\n",
        "\n",
        "### Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
        "\n",
        "Bayes' theorem is derived from the definition of conditional probability. It essentially reverses the conditional probability. While conditional probability \\( P(A|B) \\) describes the probability of \\( A \\) occurring given that \\( B \\) has occurred, Bayes' theorem allows us to compute this conditional probability in terms of the reverse conditional probability \\( P(B|A) \\), along with the individual probabilities of \\( A \\) and \\( B \\).\n",
        "\n",
        "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
        "\n",
        "The choice of which Naive Bayes classifier to use depends on the nature of the features in your dataset:\n",
        "\n",
        "1. **Gaussian Naive Bayes:** Used when the features are continuous and assume a Gaussian (normal) distribution.\n",
        "2. **Multinomial Naive Bayes:** Used when the features are discrete counts, such as word frequencies in text classification.\n",
        "3. **Bernoulli Naive Bayes:** Used when the features are binary or Boolean values, representing the presence or absence of a feature.\n",
        "\n",
        "### Q6. Naive Bayes Classification Example\n",
        "\n",
        "Given the dataset and assuming equal prior probabilities for each class, we need to calculate the probability for each class (A and B) for the new instance \\( X1 = 3 \\) and \\( X2 = 4 \\).\n",
        "\n",
        "**Step-by-Step Calculation:**\n",
        "\n",
        "1. **Calculate the prior probabilities:**\n",
        "\n",
        "\\[ P(A) = \\frac{\\text{Total instances of class A}}{\\text{Total instances}} \\]\n",
        "\\[ P(B) = \\frac{\\text{Total instances of class B}}{\\text{Total instances}} \\]\n",
        "\n",
        "Given that the table provides the frequency for each feature, the totals for class A and B are:\n",
        "- Class A: \\(3 + 3 + 4 = 10\\) instances (for \\(X1\\)) and \\(4 + 3 + 3 + 3 = 13\\) instances (for \\(X2\\))\n",
        "- Class B: \\(2 + 2 + 1 = 5\\) instances (for \\(X1\\)) and \\(2 + 2 + 2 + 3 = 9\\) instances (for \\(X2\\))\n",
        "\n",
        "Assuming equal priors:\n",
        "\\[ P(A) = P(B) = 0.5 \\]\n",
        "\n",
        "2. **Calculate the likelihoods:**\n",
        "\n",
        "\\[ P(X1 = 3 | A) = \\frac{4}{10} = 0.4 \\]\n",
        "\\[ P(X2 = 4 | A) = \\frac{3}{13} \\approx 0.231 \\]\n",
        "\n",
        "\\[ P(X1 = 3 | B) = \\frac{1}{5} = 0.2 \\]\n",
        "\\[ P(X2 = 4 | B) = \\frac{3}{9} \\approx 0.333 \\]\n",
        "\n",
        "3. **Calculate the posterior probabilities using Bayes' theorem:**\n",
        "\n",
        "\\[ P(A | X1 = 3, X2 = 4) \\propto P(X1 = 3 | A) \\cdot P(X2 = 4 | A) \\cdot P(A) \\]\n",
        "\\[ P(A | X1 = 3, X2 = 4) \\propto 0.4 \\cdot 0.231 \\cdot 0.5 \\approx 0.0462 \\]\n",
        "\n",
        "\\[ P(B | X1 = 3, X2 = 4) \\propto P(X1 = 3 | B) \\cdot P(X2 = 4 | B) \\cdot P(B) \\]\n",
        "\\[ P(B | X1 = 3, X2 = 4) \\propto 0.2 \\cdot 0.333 \\cdot 0.5 \\approx 0.0333 \\]\n",
        "\n",
        "4. **Compare the posterior probabilities:**\n",
        "\n",
        "Since \\( P(A | X1 = 3, X2 = 4) > P(B | X1 = 3, X2 = 4) \\):\n",
        "\n",
        "Naive Bayes would predict the new instance belongs to **class A**."
      ],
      "metadata": {
        "id": "Xbq3tkOtlpvG"
      }
    }
  ]
}