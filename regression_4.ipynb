{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
        "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes an L1 regularization term. This term adds the absolute values of the coefficients to the loss function, encouraging sparsity by shrinking some coefficients to exactly zero, which differs from other techniques like OLS and Ridge Regression.\n",
        "\n",
        "### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
        "The main advantage of Lasso Regression in feature selection is its ability to shrink some coefficients to exactly zero, effectively selecting a subset of features and improving model interpretability and performance by eliminating irrelevant features.\n",
        "\n",
        "### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
        "The coefficients of a Lasso Regression model indicate the relationship between each predictor and the response variable, with some coefficients possibly being exactly zero, indicating that those features are not important in the model.\n",
        "\n",
        "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
        "The primary tuning parameter in Lasso Regression is the regularization parameter (lambda). A higher lambda value increases the penalty for non-zero coefficients, leading to more coefficients being shrunk to zero, promoting sparsity and potentially reducing overfitting. The optimal lambda value is typically chosen through cross-validation.\n",
        "\n",
        "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
        "Lasso Regression can be adapted for non-linear regression problems by incorporating non-linear transformations of the predictors (e.g., polynomial features) or by using it within more complex models like Generalized Additive Models (GAMs) or in combination with kernel methods.\n",
        "\n",
        "### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
        "The main difference between Ridge Regression and Lasso Regression is in the type of regularization used. Ridge Regression uses L2 regularization (squared coefficients), which shrinks coefficients but does not set them to zero. Lasso Regression uses L1 regularization (absolute coefficients), which can shrink coefficients to zero, performing both regularization and feature selection.\n",
        "\n",
        "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
        "Yes, Lasso Regression can handle multicollinearity by shrinking some of the correlated predictors' coefficients to zero, thus reducing the complexity of the model and improving its generalizability.\n",
        "\n",
        "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
        "The optimal value of the regularization parameter (lambda) in Lasso Regression is typically chosen through cross-validation. This involves evaluating the model's performance for different lambda values and selecting the one that minimizes the cross-validation error."
      ],
      "metadata": {
        "id": "Qr16C_dOQHkh"
      }
    }
  ]
}