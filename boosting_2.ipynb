{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a detailed overview and implementation guide for your queries on Gradient Boosting Regression:\n",
        "\n",
        "### Q1. What is Gradient Boosting Regression?\n",
        "\n",
        "Gradient Boosting Regression is a machine learning technique that builds a predictive model by combining multiple weak learners (typically decision trees) sequentially. It focuses on correcting the errors of the previous models by using gradient descent to minimize the loss function. The key idea is to improve model accuracy by iteratively refining predictions based on the residuals (errors) of the previous models.\n",
        "\n",
        "### Q2. Implement a Simple Gradient Boosting Algorithm from Scratch\n",
        "\n",
        "Here’s a basic implementation of Gradient Boosting Regression from scratch using Python and NumPy. We’ll use a small dataset and evaluate the model’s performance with mean squared error and R-squared.\n",
        "\n",
        "**Implementation:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate a simple synthetic dataset\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 3 * X.flatten() + np.random.randn(100) * 2\n",
        "\n",
        "# Define a simple gradient boosting regressor\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        # Initialize predictions to zero\n",
        "        self.y_pred = np.zeros_like(y)\n",
        "        self.models = []\n",
        "        \n",
        "        for _ in range(self.n_estimators):\n",
        "            residual = y - self.y_pred\n",
        "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            model.fit(X, residual)\n",
        "            self.models.append(model)\n",
        "            self.y_pred += self.learning_rate * model.predict(X)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        # Sum the predictions from all models\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        for model in self.models:\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# Initialize and train the model\n",
        "model = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n",
        "```\n",
        "\n",
        "### Q3. Experiment with Different Hyperparameters\n",
        "\n",
        "To optimize the performance, you can use Grid Search or Random Search. Here’s an example using `GridSearchCV` from `scikit-learn`:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor as SklearnGBR\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Initialize and train the model with Grid Search\n",
        "gbr = SklearnGBR()\n",
        "grid_search = GridSearchCV(gbr, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best score: {-grid_search.best_score_:.2f}\")\n",
        "```\n",
        "\n",
        "### Q4. What is a Weak Learner in Gradient Boosting?\n",
        "\n",
        "A weak learner is a model that performs slightly better than random guessing. In the context of Gradient Boosting, the weak learner is typically a shallow decision tree (also known as a regression tree) that is used to correct the errors of previous models in the ensemble.\n",
        "\n",
        "### Q5. What is the Intuition Behind the Gradient Boosting Algorithm?\n",
        "\n",
        "The intuition behind Gradient Boosting is to build an ensemble of weak learners in a sequential manner, where each learner tries to correct the errors made by the previous learners. By focusing on the residuals (errors) of the previous models, Gradient Boosting effectively reduces the bias of the ensemble model, leading to improved accuracy.\n",
        "\n",
        "### Q6. How Does the Gradient Boosting Algorithm Build an Ensemble of Weak Learners?\n",
        "\n",
        "1. **Initialize Predictions**: Start with an initial model, often a simple model predicting the mean of the target values.\n",
        "2. **Fit Weak Learner**: Train a weak learner on the residuals (errors) of the current model.\n",
        "3. **Update Predictions**: Update the model’s predictions by adding the weak learner’s predictions, scaled by a learning rate.\n",
        "4. **Iterate**: Repeat the process, fitting new weak learners on the updated residuals until the specified number of learners is reached or performance improves sufficiently.\n",
        "\n",
        "### Q7. Steps Involved in Constructing the Mathematical Intuition of Gradient Boosting Algorithm\n",
        "\n",
        "1. **Initialize Model**: Start with an initial prediction, \\( F_0(x) \\), usually the mean of the target values.\n",
        "2. **Compute Residuals**: Calculate the residuals (errors) for each sample: \\( r_i = y_i - F_{m-1}(x_i) \\).\n",
        "3. **Fit Weak Learner**: Train a weak learner to predict the residuals.\n",
        "4. **Update Model**: Add the weak learner’s predictions to the current model’s predictions: \\( F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x) \\), where \\( \\eta \\) is the learning rate and \\( h_m(x) \\) is the weak learner.\n",
        "5. **Repeat**: Continue the process until the desired number of weak learners is reached.\n",
        "\n",
        "These steps and concepts cover the essentials of Gradient Boosting Regression and its implementation. Let me know if you need more detailed explanations or additional examples!"
      ],
      "metadata": {
        "id": "DA17K40f49Qk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9xh9skn4xx6"
      },
      "outputs": [],
      "source": []
    }
  ]
}