{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
        "Ridge Regression is a type of linear regression that includes a regularization term to prevent overfitting. It adds a penalty equal to the sum of the squared coefficients multiplied by a tuning parameter (lambda) to the ordinary least squares (OLS) loss function.\n",
        "\n",
        "### Q2. What are the assumptions of Ridge Regression?\n",
        "The assumptions of Ridge Regression are similar to those of OLS regression:\n",
        "1. Linearity: The relationship between the predictors and the response is linear.\n",
        "2. Independence: The observations are independent.\n",
        "3. Homoscedasticity: Constant variance of the errors.\n",
        "4. Normality: The errors are normally distributed.\n",
        "\n",
        "### Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
        "The value of lambda is typically selected using cross-validation. The goal is to find a lambda that minimizes the cross-validation error.\n",
        "\n",
        "### Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
        "Ridge Regression is not typically used for feature selection because it shrinks the coefficients towards zero but does not set any of them to exactly zero. However, it can be combined with other methods, like the Lasso (Least Absolute Shrinkage and Selection Operator), for feature selection.\n",
        "\n",
        "### Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
        "Ridge Regression performs well in the presence of multicollinearity because the regularization term helps to stabilize the coefficient estimates, reducing their variance and making them more reliable.\n",
        "\n",
        "### Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
        "Yes, Ridge Regression can handle both categorical and continuous independent variables. Categorical variables need to be converted into dummy variables (one-hot encoding) before applying Ridge Regression.\n",
        "\n",
        "### Q7. How do you interpret the coefficients of Ridge Regression?\n",
        "The coefficients of Ridge Regression represent the relationship between each predictor and the response variable, adjusted for the regularization penalty. They are typically smaller in magnitude compared to those from OLS regression due to the shrinkage effect.\n",
        "\n",
        "### Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
        "Yes, Ridge Regression can be used for time-series data analysis. The data should be prepared to address time-series specifics, such as autocorrelation and non-stationarity. Regularization helps to handle potential overfitting issues that may arise due to the high dimensionality of time-series data."
      ],
      "metadata": {
        "id": "Qr16C_dOQHkh"
      }
    }
  ]
}