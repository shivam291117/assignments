{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\\### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
        "\n",
        "**Answer:**\n",
        "- **Concept:** R-squared measures the proportion of the variance in the response variable that is explained by the predictor variables.\n",
        "- **Calculation:**\n",
        "  \\[\n",
        "  R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}\n",
        "  \\]\n",
        "  where:\n",
        "  - \\( \\text{SS}_{\\text{res}} \\) is the sum of squared residuals.\n",
        "  - \\( \\text{SS}_{\\text{tot}} \\) is the total sum of squares.\n",
        "- **Representation:** R-squared ranges from 0 to 1, with higher values indicating a better fit.\n",
        "\n",
        "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
        "\n",
        "**Answer:**\n",
        "- **Definition:** Adjusted R-squared adjusts R-squared for the number of predictors in the model.\n",
        "- **Difference:** Unlike R-squared, which can increase with more predictors, adjusted R-squared can decrease if the predictors do not improve the model sufficiently.\n",
        "\n",
        "### Q3. When is it more appropriate to use adjusted R-squared?\n",
        "\n",
        "**Answer:**\n",
        "- **Use:** When comparing models with different numbers of predictors.\n",
        "- **Reason:** It accounts for the number of predictors, preventing the inflation of R-squared due to excessive predictors.\n",
        "\n",
        "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
        "\n",
        "**Answer:**\n",
        "- **RMSE (Root Mean Squared Error):**\n",
        "  - **Calculation:**\n",
        "    \\[\n",
        "    \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
        "    \\]\n",
        "  - **Represents:** Average magnitude of prediction errors, with penalties for larger errors.\n",
        "\n",
        "- **MSE (Mean Squared Error):**\n",
        "  - **Calculation:**\n",
        "    \\[\n",
        "    \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "    \\]\n",
        "  - **Represents:** Average squared prediction errors.\n",
        "\n",
        "- **MAE (Mean Absolute Error):**\n",
        "  - **Calculation:**\n",
        "    \\[\n",
        "    \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
        "    \\]\n",
        "  - **Represents:** Average absolute prediction errors.\n",
        "\n",
        "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
        "\n",
        "**Answer:**\n",
        "- **RMSE:**\n",
        "  - **Advantage:** Sensitive to large errors; useful when large errors are particularly undesirable.\n",
        "  - **Disadvantage:** Penalizes large errors more heavily; not robust to outliers.\n",
        "\n",
        "- **MSE:**\n",
        "  - **Advantage:** Provides a measure of average error squared; sensitive to large errors.\n",
        "  - **Disadvantage:** Difficult to interpret due to squared units; sensitive to outliers.\n",
        "\n",
        "- **MAE:**\n",
        "  - **Advantage:** More robust to outliers; easy to interpret.\n",
        "  - **Disadvantage:** Does not penalize large errors as strongly as RMSE.\n",
        "\n",
        "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
        "\n",
        "**Answer:**\n",
        "- **Lasso Regularization:** Adds a penalty proportional to the absolute values of the coefficients (\\( \\lambda \\sum |\\beta_j| \\)).\n",
        "- **Difference from Ridge:** Lasso can shrink some coefficients to zero, performing variable selection. Ridge adds a penalty proportional to the squared values of the coefficients.\n",
        "- **Use:** Lasso is preferred when variable selection is desired or when there are many predictors.\n",
        "\n",
        "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
        "\n",
        "**Answer:**\n",
        "- **Prevention of Overfitting:** Regularized models add a penalty to the magnitude of coefficients, discouraging complex models and reducing variance.\n",
        "- **Example:** In a polynomial regression model, Ridge regularization can prevent overfitting by limiting the size of the polynomial coefficients.\n",
        "\n",
        "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
        "\n",
        "**Answer:**\n",
        "- **Limitations:**\n",
        "  - **Bias:** Regularization adds bias, which can lead to underfitting.\n",
        "  - **Complexity:** Choice of regularization parameter is crucial and can be challenging.\n",
        "  - **Not Suitable for All Types of Data:** May not perform well with non-linear relationships without transformation.\n",
        "\n",
        "### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
        "\n",
        "**Answer:**\n",
        "- **Choice:** Prefer Model B (MAE of 8) if you are concerned about the average size of errors and want robustness to outliers.\n",
        "- **Limitations:** RMSE penalizes large errors more heavily, while MAE provides a simpler, less sensitive measure of average error. Choice depends on the context and importance of outlier sensitivity.\n",
        "\n",
        "### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
        "\n",
        "**Answer:**\n",
        "- **Choice:** Choose based on goals:\n",
        "  - **Ridge Regularization (Model A):** Better if you want to retain all predictors and prevent multicollinearity.\n",
        "  - **Lasso Regularization (Model B):** Better if you want to perform variable selection and simplify the model.\n",
        "- **Trade-offs:** Ridge does not perform variable selection, while Lasso can introduce bias by shrinking some coefficients to zero. Choose based on whether model simplicity or prediction accuracy is more critical.\n"
      ],
      "metadata": {
        "id": "EjbWHZmai-Te"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9oSxB3TmlxYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}